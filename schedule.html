<html>
<head>
<title> Schedule </title>
</head>
<body>
<b> When:</b> Monday,  March 26, 2012, 11:30 am<br>

<b>Where:</b> ACES 2.444<br>

<b>Title:</b> Information Equals Amortized Communication ( The Braverman-Rao Protocol)<br>

<b>Speaker:</b> Pravesh Kothari<br><br>

<b>Abstract:</b> Suppose Alice wants to send a message M to Bob who has some partial information regarding M. Can Alice do it while keeping the expected number of bits communicated close to the amount of additional information that the message M reveals to Bob? Braverman and Rao (BR) in FOCS 2011 give a protocol that solves this problem. This is a generalization of the classical Slepian-Wolf Theorem which gives a transmission protocol (no interaction) to communicate the message M while keeping the amortized communication cost close to the expected information revealed. A caveat is that the BR protocol is interactive. 
<br>
As a consequence one can prove new relationships between randomized amortized communication complexity of a function and its information complexity. Braverman and Rao show that for any fixed distribution on the inputs, the internal information cost ( the information revealed to the parties) involved in computing any relation or function using a two party interactive protocol is exactly equal to the amortized communication complexity of computing independent copies of the same relation or function. 
<br>
In this talk, we describe and analyze the BR- protocol and discuss some of the applications of the result. 

<br><br><br>
<b>(***)When: </b>  Monday, April 2, 2012, 11:30 am<br><br><b> Where:</b> ACES 2.444 <br><br><b> Speaker:</b> Matteo Pontecorvi <br><br><b>Title:</b> Cryptanalysis of AES
<br><br><b>Abstract:</b>
re than a decade after it was first introduced, AES (Advanced Encryption Standard) is still the most secure private-key encryption system available. Or is it? In this talk, we describe the AES  in details (FIPS 197). We introduce the idea of Integral Cryptanalysis by presenting a full description of the Square Attack (Knudsen, Rijmen-Daemen) on a 4-round reduced version of AES128. Finally, we briefly recall the state of art on AES security.
<br><br><br>
<br><b>(***)When: </b>Monday,  March 5, 2012, 11:30 am
<br><br>
<b>Where: </b>ACES 2.444
<br><br>
<b>Title:</b> Lower Bounds for Learning Markov Random Fields.
<br><br><b>
Speaker:</b> Rashish Tandon
<br><br>
<b>Abstract:</b> Markov Random Fields (MRFs), which is a graph of dependencies between random variables, can be used to model any probability distribution. The problem of learning an MRF, first considered by Chow and Liu, refers to recovering the associated graph of an MRF using independent and identically drawn samples (X1,...,Xn) the probability distribution it defines. Chow and Liu considered the problem when the underlying graph of the MRF is a tree.
In this talk, we shall discuss the work of Santhanam and Wainwright ( information theoretic limits for learning an MRF) which proves hardness results for learning binary MRFs. In particular, we shall look at lower bounds on the number of samples required for learning binary MRFs exactly, with high probability when the underlying graph belongs to the following classes:
1. G(p,k) - set of graphs with p vertices and atmost k edges.
2. G(p,d) - set of graphs with p vertices and degree atmost d.

<br><br><br>
<b>(***)When: </b>  Monday, February 27, 2012, 11:30 am<br><br><b> Where:</b> ACES 2.444 <br><br><b> Speaker:</b> Xue Chen <br><br><b>Title:</b> Single-Item Auction for Correlated Bidders.
<br><br><b>Abstract:</b>Myerson, in his seminal paper, gave a complete and simple characterization of an optimal mechanism for single-item auction for the case of independent bidders. A natural open question raised by this work is to analyze the case when the bidders' values are drawn from a general (possible correlated) distribution.<br>
In this talk, we will focus on how one can compute an optimal or approximately optimal mechanism for the case of correlated bidders efficiently (for example in time polynomial in the number of bidders). We will discuss a deterministic mechanism and truthful-in-expectation mechanism. At last, we will introduce a linear program for computing the optimal auction and analyze the approximation ratio of a k-look-ahead auction.
<br><br><br>

<b>(***)When: </b>  Monday, February 20, 2012, 11:30 am<br><br>

<b> Where:</b> ACES 2.444 <br><br>

<b> Speaker:</b> Harsh Pareek <br><br>

<b>Title:</b> Variational Methods in Graphical Models<br><br>

<b>Abstract:</b>
"Variational method" is an umbrella term for methods computing a function value by representing it as an optimization problem. They are often used as approximation methods.
Graphical models are graphical representations of probability distributions, where the graph encodes relationships between variables. In the last decade, a unified framework expressing graphical model inference as an optimization problem generalizing many earlier known methods has been formulated. 

In this talk, we will represent graphical model inference as an integer program and exhibit an LP relaxation for the same. In particular, we will derive the popular message passing algorithm as coordinate descent with the Lagrange Multipliers as messages.
We will also briefly discuss non-LP relaxations for graphical model inference.

Reference: Graphical Models, Exponential Families, and Variational Inference by Martin Wainwright and Micheal Jordan (2008)


<br><br><br>
<b>(***)  Monday, 13th February 2012,  11:30am</b><br><br><b>Venue: </b> ACES 2.444<br><br><b>Speaker:</b> Matteo Pontecorvi <br><br>
<b>Title: </b> A bit of <it> Trifference </it> <br><br>
<b> Abstract: </b>
We discuss the <it> Trifference<\it> problem (J. Körner, G. Simonyi, Trifference) from a combinatorial point of view. Three strings on a ternary alphabet say {0,1,2}^n are said to be trifferent if there exists a coordinate along which each of the strings has a different letter. The trifference problem is to find the largest subset of {0,1,2}^n such that every 3 strings (in the subset) differ in at least one coordinate.<br>
We provide a lower-bound on the size of such a subset using the probabilistic method, and then a refinement due to J. Körner, K. Marton, (New bounds for perfect hashing via information theory). Finally, we openly discuss some structural properties of the Trifference leading to an interesting set of open problems, and connections to other areas (Graph Theory - Hashing).

<br><br><br>

<b>(***)  Monday, 6th February 2012,  11:30am</b><br><br><b>Venue: </b> ACES 2.444<br><br><b>Speaker:</b> Eshan Chattopadhyay <br><br>
<b>Title: </b> Constructive Discrepancy Minimization 2 (Spencer's Setting ) <br><br>
<b> Abstract: </b>
We continue the previous talk on developing constructive
algorithms for discrepancy minimization for a set system (X,S) with
|X| = n and |S| = m.  Last time  we saw an algorithm for constructing
a coloring of X with discrepancy<br> (O(\sqrt {log(mn)} \lambda) 
<br>where
\lambda is the hereditary discrepancy. We show that continuing in the
same spirit and adding some new ideas, we can in fact develop an
algorithm for achieving Spencer's discrepancy bound of <br> O(\sqrt(n
log(m/n))},<br> thus making Spencer's theorem constructive.
<br>
Reference : Constructive Algorithms for Discrepancy Minimization by
Nikhil Bansal [FOCS 2010]
<br>
<br>
<br><br>
<b> (***) Monday, 30th January 2012,  11:30am</b><br><br>
<b>Venue: </b> ACES 2.444<br><br>
<b>Speaker:</b> Pravesh Kothari <br><br>
<b> Title: </b> Constructive Algorithms for Discrepancy Minimization<br><br>

<b>Abstract: </b> <!-- Noise Sensitivity measures the average amount of change in the value of a function when the input is subjected to a "small" random perturbation. Understanding noise sensitivity of different classes of boolean functions has had profound implications in learning theory and hardness amplification. In learning theory, noise sensitivity was first used by Adam Klivans, Ryan o'Donnell and Rocco Servedio in 2002 to show that intersections and thresholds of halfspaces are learnable under uniform distribution on the hypercube. Since then there have been a number of results that show learnability of boolean function classes by bounding noise sensitivity. In this talk we present the idea of noise sensitivity of boolean functions and discuss some applications in learning theory including our result on learning non-negative submodular functions.
-->
Given a ground set X and a set of some subsets S of X, the discrepancy of a coloring function f: X --> {-1, 1}  with respect to the set system (X, S) is given by
<br>
                 disc(f) = max_{ s \in S} | \sum_{ x in s} f(x) | <br>
The discrepancy of a set system (X, S) is defined as the minimum over all coloring functions f of disc(f). Given a set system (X, S) we consider the problem of finding a coloring of X with low discrepancy.
<br>
Intuitively, low discrepancy coloring corresponds to distributing +1 and -1 so that each set in the set system receives almost the same number of +1s and -1s. In terms of applications, the min discrepancy problem appears in many varied areas of both Computer Science (Computational Geometry, Comb. Optimization, Monte-Carlo simulation, Machine learning, Complexity, Pseudo-Randomness) and Mathematics (Dynamical Systems, Combinatorics, Mathematical Finance, Number Theory, Ramsey Theory, Algebra, Measure Theory,...). There are even a couple of books dedicated to the topic.
<br>
Existence of low discrepancy coloring has been known for a long time culminating in a famous and beautiful result of Spencer in 1985 known as "six standard deviations suffice". This result states that for a set system (X, S) with X of size n and S of size n, there is a coloring of (X,S) with discrepancy at most 6 \sqrt {n}. Spencer's proof however is "highly" non constructive and it has been a major challenge since then to construct a low discrepancy coloring algorithmically.
<br>
In a major breakthrough, Nikhil Bansal gave first constructive proof of Spencer's result in FOCS 2010. Bansal's result uses a cleverly designed random walk over fractional colorings of X based on solutions to certain semidefinite programs.
<br>
In this talk, we will describe first, the discrepancy problem and then outline Bansal's constructive procedure to obtain a low discrepancy coloring. 
<br>
<br><br><br> 
</body>
</html>
